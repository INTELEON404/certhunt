#!/usr/bin/env python3
import re
import argparse
import requests
import sys
import socket
import json
import time
import shutil
import itertools
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from urllib3.exceptions import InsecureRequestWarning

# DISABLE SSL WARNINGS
requests.packages.urllib3.disable_warnings(category=InsecureRequestWarning)

VERSION = "HUNTERv1.2"
HEADERS = {
    "User-Agent": "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36"
}
TIMEOUT = 40

class Colors:
    MINT    = '\033[38;5;121m'
    SKY     = '\033[38;5;117m'
    GOLD    = '\033[38;5;222m'
    CORAL   = '\033[38;5;210m'
    LAVENDER = '\033[38;5;183m'
    SILVER  = '\033[38;5;249m'
    SLATE   = '\033[38;5;241m'
    BOLD    = '\033[1m'
    ENDC    = '\033[0m'

stop_animation = False

def status_log(msg, color=Colors.SILVER):
    if sys.stdout.isatty():
        print(f"{color}{msg}{Colors.ENDC}")

def loading_animation():
    """ANIMATION STYLE: . > .. > ..."""
    for dots in itertools.cycle(['‎ ','⠏', '⠛', '⠹', '⠸', '⠼', '⠴', '⠦', '⠧', '⠇', '⠍']):
        if stop_animation:
            break
        sys.stdout.write(f"\r {Colors.GOLD}EXTRACTING DEEP DATA{dots}{Colors.ENDC}")
        sys.stdout.flush()
        time.sleep(0.4)
    sys.stdout.write("\r" + " " * 50 + "\r")

def get_banner():
    if sys.stdout.isatty():
        columns = shutil.get_terminal_size().columns
        width = max(columns, 70)
        art = [
            "░█▀▀░█▀▀░█▀▄░▀█▀░█░█░█░█░█▀█░▀█▀",
            "░█░░░█▀▀░█▀▄░░█░░█▀█░█░█░█░█░░█░",
            "░▀▀▀░▀▀▀░▀░▀░░▀░░▀░▀░▀▀▀░▀░▀░░▀░"
        ]
        print(f"{Colors.SKY}{Colors.BOLD}")
        for line in art:
            print(line.center(width))
        info = f"DEVELOPED BY INTELEON404 | VERSION {VERSION}"
        tagline = "“DEEP PASSIVE INTELLIGENCE FOR ELITE RECON”"
        print(f"\n{Colors.LAVENDER}{info.center(width)}{Colors.ENDC}")
        print(f"{Colors.SLATE}{tagline.center(width)}{Colors.ENDC}\n")

def clean_target(domain):
    domain = re.sub(r'^https?://', '', domain.lower())
    domain = domain.replace('www.', '')
    return domain.split('/')[0].strip()

def dns_resolver(domain):
    try:
        socket.gethostbyname(domain)
        return True
    except: return False

def request_api(url, is_json=True):
    try:
        resp = requests.get(url, headers=HEADERS, timeout=TIMEOUT, verify=False)
        if resp.status_code == 200:
            return resp.json() if is_json else resp.text
    except: pass
    return None

# --- AGGRESSIVE SOURCE ENGINES ---

def extract_all(text, domain):
    """AGGRESSIVE REGEX TO FIND ANYTHING ENDING WITH TARGET DOMAIN"""
    pattern = r'(?:[a-zA-Z0-9-]+\.)+' + re.escape(domain)
    matches = re.findall(pattern, str(text))
    return {m.lower() for m in matches}

def source_abuseipdb(domain):
    res = request_api(f"https://www.abuseipdb.com/whois/{domain}", is_json=False)
    return extract_all(res, domain) if res else set()

def source_alienvault(domain):
    data = request_api(f"https://otx.alienvault.com/api/v1/indicators/domain/{domain}/passive_dns")
    subs = set()
    if data and "passive_dns" in data:
        for entry in data["passive_dns"]:
            h = entry.get("hostname", "").lower()
            if h.endswith(domain): subs.add(h)
    return subs

def source_anubis(domain):
    data = request_api(f"https://jldc.me/anubis/subdomains/{domain}")
    return {s.strip().lower() for s in data if isinstance(s, str) and s.endswith(domain)} if data else set()

def source_bevigil(domain):
    data = request_api(f"https://osint.bevigil.com/api/{domain}/subdomains/")
    if data and "subdomains" in data:
        return {s.lower() for s in data["subdomains"]}
    return set()

def source_bufferover(domain):
    data = request_api(f"https://dns.bufferover.run/dns?q=.{domain}")
    subs = set()
    if data and data.get("FDNS_A"):
        for entry in data["FDNS_A"]:
            if "," in entry: subs.add(entry.split(",")[1].lower())
    return subs

def source_certspotter(domain):
    data = request_api(f"https://api.certspotter.com/v1/issuances?domain={domain}&include_subdomains=true&expand=dns_names")
    subs = set()
    if data:
        for entry in data:
            for name in entry.get("dns_names", []):
                clean = name.replace('*.', '').lower()
                if clean.endswith(domain): subs.add(clean)
    return subs

def source_commoncrawl(domain):
    url = f"http://index.commoncrawl.org/CC-MAIN-2023-50-index?url=*.{domain}/*&output=json"
    res = request_api(url, is_json=False)
    return extract_all(res, domain) if res else set()

def source_crtsh(domain):
    data = request_api(f"https://crt.sh/?q=%.{domain}&output=json")
    subs = set()
    if data:
        for item in data:
            for name in item.get("name_value", "").split("\n"):
                clean = name.replace('*.', '').strip().lower()
                if clean.endswith(domain): subs.add(clean)
    return subs

def source_fullhunt(domain):
    data = request_api(f"https://fullhunt.io/api/v1/domain/{domain}/subdomains")
    if data and "hosts" in data:
        return {s.lower() for s in data["hosts"]}
    return set()

def source_hackertarget(domain):
    res = request_api(f"https://api.hackertarget.com/hostsearch/?q={domain}", is_json=False)
    return extract_all(res, domain) if res else set()

def source_netcraft(domain):
    res = request_api(f"https://searchdns.netcraft.com/?restriction=site+ends+with&host={domain}", is_json=False)
    return extract_all(res, domain) if res else set()

def source_omnisint(domain):
    data = request_api(f"https://sonar.omnisint.io/all/{domain}")
    return {s.lower() for s in data if isinstance(s, str)} if data else set()

def source_rapiddns(domain):
    res = request_api(f"https://rapiddns.io/s/{domain}?full=1&down=1", is_json=False)
    return extract_all(res, domain) if res else set()

def source_sitedossier(domain):
    res = request_api(f"http://www.sitedossier.com/parentdomain/{domain}", is_json=False)
    return extract_all(res, domain) if res else set()

def source_subdomaincenter(domain):
    data = request_api(f"https://api.subdomain.center/?domain={domain}")
    return {s.lower() for s in data if isinstance(s, str)} if data else set()

def source_synapsint(domain):
    data = request_api(f"https://synapsint.com/report.php?domain={domain}", is_json=False)
    return extract_all(data, domain) if data else set()

def source_urlscan(domain):
    data = request_api(f"https://urlscan.io/api/v1/search/?q=domain:{domain}")
    subs = set()
    if data and "results" in data:
        for item in data["results"]:
            sub = item.get("page", {}).get("domain", "").lower()
            if sub.endswith(domain): subs.add(sub)
    return subs

def source_virustotal(domain):
    data = request_api(f"https://www.virustotal.com/ui/domains/{domain}/subdomains?limit=40")
    subs = set()
    if data and "data" in data:
        for entry in data["data"]: subs.add(entry.get("id", "").lower())
    return subs

def source_wayback(domain):
    url = f"https://web.archive.org/cdx/search/cdx?url=*.{domain}/*&output=json&collapse=urlkey&fl=original"
    res = request_api(url, is_json=False)
    return extract_all(res, domain) if res else set()

# --- EXECUTION ENGINE ---

def main():
    global stop_animation
    parser = argparse.ArgumentParser(description="HUNTER MAX DEPTH")
    parser.add_argument("-d", "--domain", required=True, help="TARGET DOMAIN")
    parser.add_argument("-v", "--verify", action="store_true", help="VERIFY LIVE STATUS")
    parser.add_argument("-t", "--threads", type=int, default=60, help="THREADS")
    args = parser.parse_args()

    target = clean_target(args.domain)
    get_banner()

    status_log(f"[*] TARGETING : {target.upper()}", Colors.SKY)
    status_log(f"[*] MODE      : AGGRESSIVE DEPTH RECON ACTIVATED", Colors.GOLD)
    status_log(f"{Colors.SLATE}{'-' * 60}{Colors.ENDC}")

    engines = [
        source_abuseipdb, source_alienvault, source_anubis, source_bevigil,
        source_bufferover, source_certspotter, source_commoncrawl, source_crtsh,
        source_fullhunt, source_hackertarget, source_netcraft, source_omnisint,
        source_rapiddns, source_sitedossier, source_subdomaincenter,
        source_synapsint, source_urlscan, source_virustotal, source_wayback
    ]

    discovered = set()
    results_map = {}
    
    if sys.stdout.isatty():
        anim_thread = threading.Thread(target=loading_animation)
        anim_thread.start()

    with ThreadPoolExecutor(max_workers=len(engines)) as executor:
        task_map = {executor.submit(eng, target): eng.__name__.replace('source_', '').upper() for eng in engines}
        for task in as_completed(task_map):
            name = task_map[task]
            try:
                found = task.result()
                results_map[name] = found
                if found: discovered.update(found)
            except: results_map[name] = None

    stop_animation = True
    if sys.stdout.isatty():
        anim_thread.join()

    # ALPHABETICAL SUMMARY
    for name in sorted(results_map.keys()):
        found = results_map[name]
        count = len(found) if found else 0
        if count > 0:
            status_log(f" {Colors.MINT}[✓]{Colors.ENDC} {name:<18} : {Colors.BOLD}{count}{Colors.ENDC} FOUND", Colors.SILVER)
        else:
            status_log(f" {Colors.CORAL}[✗]{Colors.ENDC} {name:<18} : 0 FOUND", Colors.SLATE)

    # DEDUPLICATION & FINAL FILTER
    final_list = {s.strip().lower().rstrip('.') for s in discovered if s.endswith(target) and s != target}
    final_list = sorted(list(final_list))

    status_log(f"{Colors.SLATE}{'-' * 60}{Colors.ENDC}")
    status_log(f" {Colors.LAVENDER}[★] TOTAL UNIQUE DISCOVERED: {len(final_list)}{Colors.ENDC}", Colors.BOLD)

    if args.verify and final_list:
        status_log(f" [*] VERIFYING LIVE STATUS FOR {len(final_list)} HOSTS...", Colors.GOLD)
        live = []
        with ThreadPoolExecutor(max_workers=args.threads) as v_exec:
            v_tasks = {v_exec.submit(dns_resolver, s): s for s in final_list}
            for vt in as_completed(v_tasks):
                if vt.result(): live.append(v_tasks[vt])
        final_list = sorted(live)
        status_log(f" {Colors.MINT}[✓] TOTAL LIVE HOSTS FOUND: {len(final_list)}{Colors.ENDC}", Colors.BOLD)

    if not sys.stdout.isatty():
        for s in final_list: sys.stdout.write(s + "\n")
    else:
        if final_list:
            print(f"\n{Colors.SKY}{Colors.BOLD}{'='*25} RESULTS {'='*25}{Colors.ENDC}")
            for s in final_list:
                print(f" {Colors.MINT}→{Colors.ENDC} {Colors.SILVER}{s}{Colors.ENDC}")
            print(f"{Colors.SKY}{Colors.BOLD}{'='*59}{Colors.ENDC}")
            save_opt = input(f"\n{Colors.GOLD}[?] EXPORT RESULTS? (Y/N): {Colors.ENDC}").lower()
            if save_opt == 'y':
                fname = input(f"{Colors.GOLD}[?] FILENAME: {Colors.ENDC}") or f"hunter_{target}.txt"
                if not fname.endswith(".txt"): fname += ".txt"
                with open(fname, "w") as f: f.write("\n".join(final_list))
                print(f" {Colors.MINT}[✓] DATA EXPORTED TO: {fname}{Colors.ENDC}")

    status_log(f"\n {Colors.SKY}[!] SCAN COMPLETED. HAPPY HUNTING!{Colors.ENDC}")

if __name__ == "__main__":
    try: main()
    except KeyboardInterrupt:
        stop_animation = True
        status_log("\n [!] SESSION TERMINATED.", Colors.CORAL)
        sys.exit(0)
